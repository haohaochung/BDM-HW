{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) In news data, count the words in two fields: ‘Title’ and ‘Headline’ respectively, and list the most frequent words according to the term frequency in descending order, in total, per day, and per topic, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+---------+-------------------+-------------------+--------------------+--------+----------+--------+\n",
      "|IDLink|               Title|            Headline|              Source|    Topic|        PublishDate|     SentimentTitle|   SentimentHeadline|Facebook|GooglePlus|LinkedIn|\n",
      "+------+--------------------+--------------------+--------------------+---------+-------------------+-------------------+--------------------+--------+----------+--------+\n",
      "| 99248|Obama Lays Wreath...|Obama Lays Wreath...|           USA TODAY|    obama|2002-04-02 00:00:00|                0.0| -0.0533001790889026|      -1|        -1|      -1|\n",
      "| 10423|A Look at the Hea...|Tim Haywood, inve...|           Bloomberg|  economy|2008-09-20 00:00:00|  0.208333333333333|  -0.156385810542806|      -1|        -1|      -1|\n",
      "| 18828|Nouriel Roubini: ...|Nouriel Roubini, ...|           Bloomberg|  economy|2012-01-28 00:00:00| -0.425210032135381|   0.139754248593737|      -1|        -1|      -1|\n",
      "| 27788|Finland GDP Expan...|Finland's economy...|            RTT News|  economy|2015-03-01 00:06:00|                0.0|  0.0260643017571343|      -1|        -1|      -1|\n",
      "| 27789|Tourism, govt spe...|Tourism and publi...|The Nation - Thai...|  economy|2015-03-01 00:11:00|                0.0|   0.141084456488315|      -1|        -1|      -1|\n",
      "| 27790|Intellitec Soluti...|Over 100 attendee...|               PRWeb|microsoft|2015-03-01 00:19:00|-0.0753778361444409|  0.0367727935198561|      -1|        -1|      -1|\n",
      "| 80690| Monday, 29 Feb 2016|RAMALLAH, Februar...|                null|palestine|2016-02-28 14:03:00|                0.0|-0.00590569489076917|       0|         0|       0|\n",
      "| 80762|Obama, stars pay ...|First lady Michel...|      Coast Reporter|    obama|2015-03-01 00:45:00| 0.0833333333333333|   0.103002756765217|      -1|        -1|      -1|\n",
      "| 80771|Fire claims more ...|A Hancock County ...|   WTHR Indianapolis|palestine|2015-03-01 01:20:00| -0.173925271309261| -0.0501846917841854|      -1|        -1|      -1|\n",
      "| 27803|Microsoft's new W...|New Delhi, Feb.29...|          New Kerala|microsoft|2015-03-01 01:32:00|-0.0595362090259799| -0.0817150563075766|      -1|        -1|      -1|\n",
      "| 27813|Microsoft Project...|Microsoft may hav...|           SlashGear|microsoft|2015-03-01 02:14:00|                0.0| 0.00254994083794926|      -1|        -1|      -1|\n",
      "| 27812|Microsoft sneaks ...|The platform batt...|        The Register|microsoft|2015-03-01 02:15:00|                0.0|  0.0526702630059281|      -1|        -1|      -1|\n",
      "| 27838|Greek economy gro...|Greece's economy ...|Reuters via Yahoo...|  economy|2015-03-01 02:16:00|                0.0|  -0.375259329232792|      -1|        -1|      -1|\n",
      "| 27811|Big data and the ...|Big data analytic...|     Information Age|  economy|2015-03-01 02:18:00| 0.0632049365526906|  0.0389861416984116|      -1|        -1|      -1|\n",
      "| 27814|HoloLens dev edit...|Microsoft’s AR he...|               Metro|microsoft|2015-03-01 02:18:00|                0.0|  0.0794336845206687|      -1|        -1|      -1|\n",
      "| 27823|Microsoft Word fo...|What is A + B? We...|         Macworld UK|microsoft|2015-03-01 02:54:00|-0.0238833973016124| -0.0322748612183951|      -1|        -1|      -1|\n",
      "| 27827|Microsoft Band 2 ...|The Microsoft Ban...|           SlashGear|microsoft|2015-03-01 03:10:00|-0.0708683386892301|                 0.0|      -1|        -1|      -1|\n",
      "| 27828|Microsoft prepare...|It seems that Mic...|           TechRadar|microsoft|2015-03-01 03:10:00|                0.0|        0.0013671875|      -1|        -1|      -1|\n",
      "| 27826|Greek economy shr...|Greece's economy ...|Reuters via Yahoo...|  economy|2015-03-01 03:15:00|-0.0856450199628935| -0.0995312715398377|      -1|        -1|      -1|\n",
      "| 27839|Sweden's economy ...|Sweden's economy ...|Business Insider ...|  economy|2015-03-01 03:15:00|                0.0|  -0.014173667737846|      -1|        -1|      -1|\n",
      "+------+--------------------+--------------------+--------------------+---------+-------------------+-------------------+--------------------+--------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "workspace='file:/home/wei/文件/研究所/BDM/HW2/'\n",
    "news_data=sqlContext.read.format('csv')\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .option(\"inferschema\", \"true\")\\\n",
    "                    .option(\"mode\", \"DROPMALFORMED\")\\\n",
    "                    .load(workspace + 'data/News_Final.csv')\n",
    "news_data=news_data.withColumn('SentimentTitle', news_data['SentimentTitle'].cast('double'))\n",
    "news_data=news_data.withColumn('SentimentHeadline', news_data['SentimentHeadline'].cast('double'))\n",
    "news_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TitleCount_total=news_data.select('Title').rdd.flatMap(list)\\\n",
    "                                   .flatMap(lambda x:x.split(' '))\\\n",
    "                                   .map(lambda x:(x, 1))\\\n",
    "                                   .reduceByKey(add)\\\n",
    "                                   .sortBy(lambda x:x[1], False)\\\n",
    "                                   .collect()\n",
    "out=open('output/TitleCount_total.txt','w')\n",
    "for i in range(len(TitleCount_total)):\n",
    "    out.write(str(TitleCount_total[i]) + '\\n')\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HeadlineCount_total=news_data.select('Headline').rdd.flatMap(list)\\\n",
    "                                         .filter(lambda x:type(x)==str)\\\n",
    "                                         .flatMap(lambda x:x.split(' '))\\\n",
    "                                         .map(lambda x:(x, 1))\\\n",
    "                                         .reduceByKey(add)\\\n",
    "                                         .sortBy(lambda x:x[1], False)\\\n",
    "                                         .collect()\n",
    "out=open('output/HeadlineCount_total.txt','w')\n",
    "for i in range(len(HeadlineCount_total)):\n",
    "    out.write(str(HeadlineCount_total[i]) + '\\n')\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TitleCount_day=news_data.select('PublishDate', 'Title').rdd.map(list)\\\n",
    "                                                .map(lambda x:(x[0].split(' ')[0].replace('-', '') ,x[1].split(' ')))\\\n",
    "                                                .flatMap(lambda x:[(x[0], element) for element in x[1]])\\\n",
    "                                                .map(lambda x:(x, 1))\\\n",
    "                                                .reduceByKey(add)\\\n",
    "                                                .sortBy(lambda x:(x[0][0], x[1]), False)\\\n",
    "                                                .collect()\n",
    "out=open('output/TitleCount_day.txt','w')\n",
    "currentDate=TitleCount_day[0][0][0]\n",
    "for i in range(len(TitleCount_day)):\n",
    "    if currentDate!=TitleCount_day[i][0][0]:\n",
    "        out.write('\\n')\n",
    "        currentDate=TitleCount_day[i][0][0]\n",
    "    out.write(str(TitleCount_day[i]) + '\\n')\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HeadlineCount_day=news_data.select('PublishDate', 'Headline').rdd.map(list)\\\n",
    "                                                      .filter(lambda x:type(x[1])==str)\\\n",
    "                                                      .map(lambda x:(x[0].split(' ')[0].replace('-', '') ,x[1].split(' ')))\\\n",
    "                                                      .flatMap(lambda x:[(x[0], element) for element in x[1]])\\\n",
    "                                                      .map(lambda x:(x, 1))\\\n",
    "                                                      .reduceByKey(add)\\\n",
    "                                                      .sortBy(lambda x:(x[0][0], x[1]), False)\\\n",
    "                                                      .collect()\n",
    "out=open('output/HeadlineCount_day.txt','w')\n",
    "currentDate=HeadlineCount_day[0][0][0]\n",
    "for i in range(len(HeadlineCount_day)):\n",
    "    if currentDate!=HeadlineCount_day[i][0][0]:\n",
    "        out.write('\\n')\n",
    "        currentDate=HeadlineCount_day[i][0][0]\n",
    "    out.write(str(HeadlineCount_day[i]) + '\\n')\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TitleCount_topic=news_data.select('Topic', 'Title').rdd.map(list)\\\n",
    "                                            .map(lambda x:(x[0] ,x[1].split(' ')))\\\n",
    "                                            .flatMap(lambda x:[(x[0], element) for element in x[1]])\\\n",
    "                                            .map(lambda x:(x, 1))\\\n",
    "                                            .reduceByKey(add)\\\n",
    "                                            .sortBy(lambda x:(x[0][0], -x[1]))\\\n",
    "                                            .collect()\n",
    "out=open('output/TitleCount_topic.txt','w')\n",
    "currentTopic=TitleCount_topic[0][0][0]\n",
    "for i in range(len(TitleCount_topic)):\n",
    "    if currentTopic!=TitleCount_topic[i][0][0]:\n",
    "        out.write('\\n')\n",
    "        currentTopic=TitleCount_topic[i][0][0]\n",
    "    out.write(str(TitleCount_topic[i]) + '\\n')\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "HeadlineCount_topic=news_data.select('Topic', 'Headline').rdd.map(list)\\\n",
    "                                                  .filter(lambda x:type(x[1])==str)\\\n",
    "                                                  .map(lambda x:(x[0] ,x[1].split(' ')))\\\n",
    "                                                  .flatMap(lambda x:[(x[0], element) for element in x[1]])\\\n",
    "                                                  .map(lambda x:(x, 1))\\\n",
    "                                                  .reduceByKey(add)\\\n",
    "                                                  .sortBy(lambda x:(x[0][0], -x[1]))\\\n",
    "                                                  .collect()\n",
    "out=open('output/HeadlineCount_topic.txt','w')\n",
    "currentTopic=HeadlineCount_topic[0][0][0]\n",
    "for i in range(len(HeadlineCount_topic)):\n",
    "    if currentTopic!=HeadlineCount_topic[i][0][0]:\n",
    "        out.write('\\n')\n",
    "        currentTopic=HeadlineCount_topic[i][0][0]\n",
    "    out.write(str(HeadlineCount_topic[i]) + '\\n')\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) In social feedback data, calculate the average popularity of each news by hour, and by day, respectively (for each platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileList=['Facebook_Economy', 'Facebook_Microsoft', 'Facebook_Obama', 'Facebook_Palestine',\\\n",
    "          'GooglePlus_Economy', 'GooglePlus_Microsoft', 'GooglePlus_Obama', 'GooglePlus_Palestine',\\\n",
    "          'LinkedIn_Economy', 'LinkedIn_Microsoft', 'LinkedIn_Obama', 'LinkedIn_Palestine']\n",
    "\n",
    "header_per_hour=['IDLink'] + ['TS'+str((count+1)*3) for count in range(48)]\n",
    "header_per_day=['IDLink'] + ['TS'+str((count+1)*72) for count in range(2)]\n",
    "\n",
    "platforms='Facebook'\n",
    "out_hour=open('output/' + platforms + '_hour.txt','w')\n",
    "out_day=open('output/' + platforms + '_day.txt','w')\n",
    "for fileName in fileList:\n",
    "    social_feedback_data=sqlContext.read.format('csv')\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferschema\", \"true\")\\\n",
    "    .load(workspace + '/data/' + fileName + '.csv')\n",
    "    popularityAvg_per_hour=social_feedback_data.select(header_per_hour).rdd.map(list)\\\n",
    "                                               .flatMap(lambda x:((x[0], element) for element in x[1:]))\\\n",
    "                                               .reduceByKey(add).map(lambda x:(x[0], x[1]/48)).sortByKey()\\\n",
    "                                               .map(lambda x:('ID'+str(x[0]), x[1])).collect()\n",
    "    popularityAvg_per_day=social_feedback_data.select(header_per_day).rdd.map(list)\\\n",
    "                                              .flatMap(lambda x:((x[0], element) for element in x[1:]))\\\n",
    "                                              .reduceByKey(add).map(lambda x:(x[0], x[1]/2)).sortByKey()\\\n",
    "                                              .map(lambda x:('ID'+str(x[0]), x[1])).collect()\n",
    "    if fileName.split('_')[0] != platforms:\n",
    "        out_hour.close()\n",
    "        out_day.close()\n",
    "        platforms=fileName.split('_')[0]\n",
    "        out_hour=open('output/' + platforms + '_hour.txt','w')\n",
    "        out_day=open('output/' + platforms + '_day.txt','w')\n",
    "    out_hour.write(fileName + '\\n')\n",
    "    out_day.write(fileName + '\\n')\n",
    "    for i in range(len(popularityAvg_per_hour)):\n",
    "        out_hour.write(str(popularityAvg_per_hour[i]) + '\\n')\n",
    "    out_hour.write('\\n')\n",
    "    for i in range(len(popularityAvg_per_day)):\n",
    "        out_day.write(str(popularityAvg_per_day[i]) + '\\n')\n",
    "    out_day.write('\\n')\n",
    "out_hour.close()\n",
    "out_day.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) In news data, calculate the sum and average sentiment score of each topic, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentimentScore=news_data.select('Topic', 'SentimentTitle', 'SentimentHeadline').rdd.map(list).map(lambda x:(x[0], [x[1], x[2], 1])).reduceByKey(lambda x, y:[x[0]+y[0], x[1]+y[1], x[2]+y[2]])\n",
    "SentimentScore_sum=SentimentScore.sortByKey(False).map(lambda x:[x[0], x[1][0], x[1][1]]).collect()\n",
    "SentimentScore_average=SentimentScore.sortByKey(False).map(lambda x:[x[0], x[1][0]/x[1][2], x[1][1]/x[1][2]]).collect()\n",
    "\n",
    "out=open('output/SentimentScore.txt','w')\n",
    "out.write('SentimentScore_sum\\n')\n",
    "for i in range(len(SentimentScore_sum)):\n",
    "    out.write(str(SentimentScore_sum[i]) + '\\n')\n",
    "out.write('\\nSentimentScore_average\\n')\n",
    "for i in range(len(SentimentScore_average)):\n",
    "    out.write(str(SentimentScore_average[i]) + '\\n')\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4) From subtask (1), for the top-100 frequent words per topic in titles and headlines, calculate their co-occurrence matrices (100x100), respectively. Each entry in the matrix will contain the co-occurrence frequency in all news titles and headlines, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=100\n",
    "topics=sorted(news_data.select('Topic').rdd.flatMap(list).distinct().collect())\n",
    "for topic in topics:\n",
    "    top_k_frequent_words=news_data.select('Topic', 'Title').rdd.map(list)\\\n",
    "                                            .map(lambda x:(x[0] ,x[1].split(' ')))\\\n",
    "                                            .flatMap(lambda x:[(x[0], element) for element in x[1]])\\\n",
    "                                            .map(lambda x:(x, 1))\\\n",
    "                                            .reduceByKey(add)\\\n",
    "                                            .sortBy(lambda x:(x[0][0], -x[1]))\\\n",
    "                                            .filter(lambda x:x[0][0]==topic)\\\n",
    "                                            .map(lambda x:x[0][1])\\\n",
    "                                            .collect()\n",
    "    top_k_frequent_words=top_k_frequent_words[:k]\n",
    "    title=news_data.select('Title', 'Topic').rdd.map(list).filter(lambda x:x[1]==topic).map(lambda x:x[0]).collect()\n",
    "    co_occurrence_matrices=[[0]*k for i in range(k)]\n",
    "    out=open('output/top_' + str(k) + '_frequent_words_co_occurrence_matrices_in_Title_of_topic_' + topic + '.txt','w')\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            for t in range(len(title)):\n",
    "                if top_k_frequent_words[i] in title[t] and top_k_frequent_words[j] in title[t]:\n",
    "                    co_occurrence_matrices[i][j]+=1\n",
    "        out.write(str(co_occurrence_matrices[i]) + '\\n')\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=100\n",
    "topics=sorted(news_data.select('Topic').rdd.flatMap(list).distinct().collect())\n",
    "for topic in topics:\n",
    "    top_k_frequent_words=news_data.select('Topic', 'Headline').rdd.map(list)\\\n",
    "                                            .filter(lambda x:type(x[1])==str)\\\n",
    "                                            .map(lambda x:(x[0] ,x[1].split(' ')))\\\n",
    "                                            .flatMap(lambda x:[(x[0], element) for element in x[1]])\\\n",
    "                                            .map(lambda x:(x, 1))\\\n",
    "                                            .reduceByKey(add)\\\n",
    "                                            .sortBy(lambda x:(x[0][0], -x[1]))\\\n",
    "                                            .filter(lambda x:x[0][0]==topic)\\\n",
    "                                            .map(lambda x:x[0][1])\\\n",
    "                                            .collect()\n",
    "    top_k_frequent_words=top_k_frequent_words[:k]\n",
    "    headline=news_data.select('Headline', 'Topic').rdd.map(list).filter(lambda x:x[1]==topic).map(lambda x:x[0]).filter(lambda x:type(x)==str).collect()\n",
    "    co_occurrence_matrices=[[0]*k for i in range(k)]\n",
    "    out=open('output/top_' + str(k) + '_frequent_words_co_occurrence_matrices_in_Headline_of_topic_' + topic + '.txt','w')\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            for t in range(len(headline)):\n",
    "                if top_k_frequent_words[i] in headline[t] and top_k_frequent_words[j] in headline[t]:\n",
    "                    co_occurrence_matrices[i][j]+=1\n",
    "        out.write(str(co_occurrence_matrices[i]) + '\\n')\n",
    "    out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
